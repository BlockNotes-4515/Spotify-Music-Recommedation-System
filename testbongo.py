# -*- coding: utf-8 -*-
"""Spotify_Music_Recommedation_System_AI_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huAkD0lvQzyR_86zoZAVuFkO02GwI7WK

# STEP-1: Data Collections.
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the Pre-Defined Libraries.
import os
import numpy as np
import pandas as pd

import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import euclidean_distances
from scipy.spatial.distance import cdist

import warnings
warnings.filterwarnings("ignore");

#Importing the Values of the Datasets.
data = pd.read_csv("data.csv");
genre_data = pd.read_csv('data_by_genres.csv');
year_data = pd.read_csv('data_by_year.csv');

#Showing the Information of about the Data.
print(data.info());

#Showing the Genre_Data Informations.
print(genre_data.info())

#Showing the Year-Wise Data Informations.
print(year_data.info());

#Now, we haveto describe the values of the main datasets.
print(data.describe());

#We, also describing the datasets values of the genre.
print(genre_data.describe());

#We, also describing the values of the Yaer-Wise Datasets.
print(year_data.describe());

"""# STEP-2: Data Processings & Data Visualizations."""

# Dropping the Unnecessary Columns.
# Check the actual column names in your DataFrame using:
print(data.columns)


#Checking the Null Values.
print("\n------------------------------------");
print("\n",data.isnull().sum())

# Identify non-numeric columns
non_numeric_cols = data.select_dtypes(exclude=['number']).columns

# Replacing the Null Values with Mean for numeric columns only
numeric_data = data.select_dtypes(include=['number'])
numeric_data = numeric_data.fillna(numeric_data.mean())

# Reassemble the DataFrame if needed
data = pd.concat([numeric_data, data[non_numeric_cols]], axis=1)

#Checking the Null Values Again.
print("\n------------------------------------");
print("\n",data.isnull().sum());

"""--> We are going to check for all the analysis with the target as 'popularity'. Before going to do that let's check for the Feature Correlation by considering a few features and for that, I'm going to use the yellowbrick package. You can learn more about it from the documentation."""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from yellowbrick.target import FeatureCorrelation

# Assuming 'data' is a DataFrame already defined and contains the necessary columns

# List of feature names
feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',
       'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'duration_ms', 'explicit', 'key', 'mode', 'year']

# Splitting the data into features (X) and target (y)
X, y = data[feature_names], data['popularity']

# Handle missing values by filling them with the mean of each column
X = X.fillna(X.mean())
y = y.fillna(y.mean())

# Ensure there are no infinite values
X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.mean())

# Convert feature names to a numpy array (although not strictly necessary)
features = np.array(feature_names)

# Instantiate the visualizer with the feature labels
visualizer = FeatureCorrelation(labels=features)

# Set the figure size
plt.rcParams['figure.figsize'] = (20, 20);

# Fit the data to the visualizer
visualizer.fit(X, y);

# Display the visualizer
# visualizer.show();

"""--> Data Understanding by Visualization and EDA

--> Music Over Time¶
Using the data grouped by year, we can understand how the overall sound of music has changed from 1921 to 2020.

#EDA-Exploratory Data Analysis.
"""

def get_decade(year):
    period_start = int(year/10) * 10
    decade = '{}s'.format(period_start)
    return decade

data['decade'] = data['year'].apply(get_decade)

sns.set(rc={'figure.figsize':(11 ,6)})
sns.countplot(data['decade']);

data['decade'] = data['year'].apply(get_decade)
sns.set(rc={'figure.figsize': (11, 6)})
sns.countplot(data['decade'])
sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(year_data, x='year', y=sound_features)
# fig.show()
top10_genres = genre_data.nlargest(10, 'popularity')
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')
# fig.show()

sound_features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'valence']
fig = px.line(year_data, x='year', y=sound_features)
# fig.show()

"""#Characteristics of Different Genres¶

--> This dataset contains the audio features for different songs along with the audio features for different genres. We can use this information to compare different genres and understand their unique differences in sound.
"""

top10_genres = genre_data.nlargest(10, 'popularity');
fig = px.bar(top10_genres, x='genres', y=['valence', 'energy', 'danceability', 'acousticness'], barmode='group')
# fig.show();

"""# Step-3: Clustering Genres with K-Means¶

-->Here, the simple K-means clustering algorithm is used to divide the genres in this dataset into ten clusters based on the numerical audio features of each genres.

--> This code snippet is used to perform clustering on a dataset (genre_data) using the K-means algorithm within a scikit-learn Pipeline. Here's a breakdown of what each part of the code does:
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Removed n_jobs parameter
cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=10))])
X = genre_data.select_dtypes(np.number)
cluster_pipeline.fit(X)
genre_data['cluster'] = cluster_pipeline.predict(X);

# Visualizing the Clusters with t-SNE

from sklearn.manifold import TSNE

tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, verbose=1))])
genre_embedding = tsne_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)
projection['genres'] = genre_data['genres']
projection['cluster'] = genre_data['cluster']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genres'])
# fig.show();

"""#Clustering Songs with K-Means"""

# Import necessary libraries
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import numpy as np

# Assuming 'data' is a DataFrame already defined and contains the necessary columns

# Create the pipeline
song_cluster_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('kmeans', KMeans(n_clusters=20, verbose=False, n_init=10)) # Removed n_jobs
], verbose=False)

# Select numerical columns
X = data.select_dtypes(np.number)
number_cols = list(X.columns)

# Fit the pipeline to the data
song_cluster_pipeline.fit(X)

# Predict cluster labels
song_cluster_labels = song_cluster_pipeline.predict(X)

# Add cluster labels to the DataFrame
data['cluster_label'] = song_cluster_labels

# Visualizing the Clusters with PCA

from sklearn.decomposition import PCA

pca_pipeline = Pipeline([('scaler', StandardScaler()), ('PCA', PCA(n_components=2))])
song_embedding = pca_pipeline.fit_transform(X)
projection = pd.DataFrame(columns=['x', 'y'], data=song_embedding)
projection['title'] = data['name']
projection['cluster'] = data['cluster_label']

fig = px.scatter(
    projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'title'])
# fig.show();

"""# STEP-4: Calculating the Overall Training and Testing Accuracy."""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Assuming 'popularity' is the target variable and you want to predict it
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['popularity', 'name', 'artists', 'id', 'release_date', 'decade', 'cluster_label'], axis=1),
    data['popularity'], test_size=0.2, random_state=42
)

# Impute missing values in the target variable (y_train)
imputer = SimpleImputer(strategy='mean')
y_train = imputer.fit_transform(y_train.values.reshape(-1, 1))
y_train = y_train.ravel()

# If 'popularity' is a continuous variable, consider using a regression model
from sklearn.linear_model import LinearRegression

# Create a pipeline to impute missing values and then apply the model
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('model', LinearRegression())
])

# Fit the pipeline
pipeline.fit(X_train, y_train)

# Make predictions using the pipeline
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# Evaluate the model using appropriate metrics for regression
from sklearn.metrics import mean_squared_error, r2_score

print("Mean Squared Error: {:.2f}".format(mean_squared_error(y_test, y_pred_test)))
print("R-squared: {:.2f}".format(r2_score(y_test, y_pred_test)))

# For calculating accuracy, you need to convert the continuous predictions to discrete classes
# This requires defining a threshold or using a different metric more suitable for regression
# Here's an example using a simple threshold of 50 for demonstration:
threshold = 50
y_pred_train_class = (y_pred_train > threshold).astype(int)
y_pred_test_class = (y_pred_test > threshold).astype(int)
y_train_class = (y_train > threshold).astype(int)
y_test_class = (y_test > threshold).astype(int)

train_accuracy = accuracy_score(y_train_class, y_pred_train_class)
test_accuracy = accuracy_score(y_test_class, y_pred_test_class)

print("Training Accuracy: {:.2f}".format(train_accuracy));
print("Testing Accuracy: {:.2f}".format(test_accuracy));

"""# STEP-5 : Implementing the Value of CNN-Neural Networking (Epochs)."""

# prompt: apply cnn to increase the training and testing accuracy

import tensorflow as tf;
from tensorflow.keras.models import Sequential;
from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout;
from sklearn.preprocessing import MinMaxScaler;

# Assuming 'popularity' is the target variable and you want to predict it
X = data.drop(['popularity', 'name', 'artists', 'id', 'release_date', 'decade', 'cluster_label'], axis=1)
y = data['popularity']

# Impute missing values in the features (X)
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

# Scale features to the range [0, 1]
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Reshape data for CNN (samples, timesteps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the CNN model
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))  # Output layer for regression

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

print("Mean Squared Error (Train): {:.2f}".format(mean_squared_error(y_train, y_pred_train)))
print("Mean Squared Error (Test): {:.2f}".format(mean_squared_error(y_test, y_pred_test)))
print("R-squared (Train): {:.2f}".format(r2_score(y_train, y_pred_train)))
print("R-squared (Test): {:.2f}".format(r2_score(y_test, y_pred_test)))

# For calculating accuracy, you still need to convert continuous predictions to classes
# using a threshold or a different metric suitable for regression.

"""# >> Implementing the Values of the CNN to increase the Overall The Training Accuracy."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Assuming df is your DataFrame
features = ['danceability', 'energy', 'loudness', 'valence', 'tempo']

# Create a binary target column based on popularity
data['target'] = (data['popularity'] > 50).astype(int)  # Adjust the threshold as needed

# Define feature matrix X and target vector y
X = data[features]
y = data['target']

# Normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training (70%) and testing (30%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the neural network model
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.5))  # Add dropout layer to prevent overfitting
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification if needed

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Use 'categorical_crossentropy' for multi-class classification

# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Custom callback to print training and validation accuracy
class PrintAccuracyCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        train_acc = logs.get('accuracy')
        val_acc = logs.get('val_accuracy')
        print(f"Epoch {epoch + 1}: Training Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}")

# Train the model
history = model.fit(X_train, y_train, epochs=1, validation_split=0.2, callbacks=[early_stopping, PrintAccuracyCallback()])

# Make predictions
y_pred_proba = model.predict(X_test)  # Get prediction probabilities
y_pred = (y_pred_proba > 0.5).astype("int32")  # Convert probabilities to binary predictions

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(report);

"""--> Based on this code print the f1 score, precision, recall, traing and testing accuracy in tabular form"""

# prompt: based on this code print the f1 score, precision, recall, traing and testing accuracy in tabular form

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# ... (Your existing code) ...

# Make predictions
y_pred_proba = model.predict(X_test)  # Get prediction probabilities
y_pred = (y_pred_proba > 0.5).astype("int32")  # Convert probabilities to binary predictions

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred);
precision = precision_score(y_test, y_pred);
recall = recall_score(y_test, y_pred);
f1 = f1_score(y_test, y_pred);

# Extract training and validation accuracy from history
train_accuracy = history.history['accuracy'][-1]
val_accuracy = history.history['val_accuracy'][-1]

# Create a DataFrame for the results
results_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Training Accuracy', 'Validation Accuracy'],
    'Value': [accuracy, precision, recall, f1, train_accuracy, val_accuracy]
})

# Display the results in tabular form
print("\n------------------",results_df.to_string(index=False));

"""# STEP-5: Builiding a Recommedation Engine.

--> Based on the analysis and visualizations, it’s clear that similar genres tend to have data points that are located close to each other while similar types of songs are also clustered together.

--> This observation makes perfect sense. Similar genres will sound similar and will come from similar time periods while the same can be said for songs within those genres. We can use this idea to build a recommendation system by taking the data points of the songs a user has listened to and recommending songs corresponding to nearby data points.

--> Spotipy is a Python client for the Spotify Web API that makes it easy for developers to fetch data and query Spotify’s catalog for songs. You have to install using pip install spotipy.

--> After installing Spotipy, you will need to create an app on the Spotify Developer’s page and save your Client ID and secret key.
"""

# !pip install spotipy

import os;
# Spotify credentials
os.environ["SPOTIFY_CLIENT_ID"] = "a5ad8b6ab10f4e969227a8b8982d9ecd";
os.environ["SPOTIFY_CLIENT_SECRET"] = "6f6e351c549346d297c9b76cb14587fd";
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from collections import defaultdict

sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.environ["SPOTIFY_CLIENT_ID"],
                                                           client_secret=os.environ["SPOTIFY_CLIENT_SECRET"]))

def find_song(name, year):
    song_data = defaultdict()
    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    song_data['name'] = [name]
    song_data['year'] = [year]
    song_data['explicit'] = [int(results['explicit'])]
    song_data['duration_ms'] = [results['duration_ms']]
    song_data['popularity'] = [results['popularity']]

    for key, value in audio_features.items():
        song_data[key] = value

    return pd.DataFrame(song_data)

from collections import defaultdict
from sklearn.metrics import euclidean_distances
from scipy.spatial.distance import cdist
import difflib

number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',
 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']


def get_song_data(song, spotify_data):

    try:
        song_data = spotify_data[(spotify_data['name'] == song['name'])
                                & (spotify_data['year'] == song['year'])].iloc[0]
        return song_data

    except IndexError:
        return find_song(song['name'], song['year'])


def get_mean_vector(song_list, spotify_data):

    song_vectors = []

    for song in song_list:
        song_data = get_song_data(song, spotify_data)
        if song_data is None:
            print('Warning: {} does not exist in Spotify or in database'.format(song['name']))
            continue
        song_vector = song_data[number_cols].values
        song_vectors.append(song_vector)

    song_matrix = np.array(list(song_vectors))
    return np.mean(song_matrix, axis=0)


def flatten_dict_list(dict_list):

    flattened_dict = defaultdict()
    for key in dict_list[0].keys():
        flattened_dict[key] = []

    for dictionary in dict_list:
        for key, value in dictionary.items():
            flattened_dict[key].append(value)

    return flattened_dict


def recommend_songs( song_list, spotify_data, n_songs=10):

    metadata_cols = ['name', 'year', 'artists']
    song_dict = flatten_dict_list(song_list)

    song_center = get_mean_vector(song_list, spotify_data)
    scaler = song_cluster_pipeline.steps[0][1]
    scaled_data = scaler.transform(spotify_data[number_cols])
    scaled_song_center = scaler.transform(song_center.reshape(1, -1))
    distances = cdist(scaled_song_center, scaled_data, 'cosine')
    index = list(np.argsort(distances)[:, :n_songs][0])

    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
    return rec_songs[metadata_cols].to_dict(orient='records')

"""# STEP-6 : To take from the User's Input!"""

# def recommend_songs( song_list, spotify_data, n_songs=10):

#     metadata_cols = ['name', 'year', 'artists']
#     song_dict = flatten_dict_list(song_list)

#     song_center = get_mean_vector(song_list, spotify_data)
#     scaler = song_cluster_pipeline.steps[0][1]
#     scaled_data = scaler.transform(spotify_data[number_cols])
#     scaled_song_center = scaler.transform(song_center.reshape(1, -1))
#     distances = cdist(scaled_song_center, scaled_data, 'cosine')
#     index = list(np.argsort(distances)[:, :n_songs][0])

#     rec_songs = spotify_data.iloc[index]
#     rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
#     return rec_songs[metadata_cols].to_dict(orient='records')
# # Function to get user input
# def get_user_input():
#     print("\n======================= WELCOME TO THE SPOTIFY (MUSICX) ====================")
#     print("Please register:")
#     name = input("\n 1. Enter your name: ")
#     age = int(input("\n 2. Enter your age: "))
#     gender = input("\n 3. Enter your gender (M/F): ")

#     print("\n 4. Select Language:")
#     print("a. Hindi")
#     print("b. English")
#     print("c. Other");
#     language_option = input("\n 5. Enter your choice (a/b/c): ").strip().lower()
#     language = {
#         'a': 'Hindi',
#         'b': 'English',
#         'c': 'Other'
#     }.get(language_option, 'Other')

#     print("\n 6. Select your preferred song type:")
#     print("a. Rock")
#     print("b. Romantic")
#     print("c. Neon")
#     print("d. Peace")
#     print("e. Hip-Hop")
#     type_option = input("\n 7. Enter your choice (a/b/c/d/e): ").strip().lower()
#     song_type = {
#         'a': 'Rock',
#         'b': 'Romantic',
#         'c': 'Neon',
#         'd': 'Peace',
#         'e': 'Hip-Hop'
#     }.get(type_option, 'Rock')  # Default to 'Rock' if invalid input

#     # Confirmation step
#     print("\n Do you allow to submit (Y/N): ")
#     if input().strip().upper() == 'Y':
#         print("\n============================ Registered Successfully! ======================")
#     else:
#         print("\n Not Registered Only Free Musics Available without Premium")
#         exit()
#     print("\n=================================================================================");
#     print("\n=================================================================================");


#     song_list = []
#     while True:
#         song_name = input("\n ----> Enter the name of a song you like (or type 'done' to finish): ")
#         if song_name.lower() == 'done':
#             break
#         artist_name = input("\n 1. Enter the name of the artist: ")
#         song_year = int(input("\n 2. Enter the release year of the song: "))
#         song_list.append({'name': song_name, 'year': song_year, 'artists': artist_name})
#     return song_list

# # Main function
# def main():
#     user_songs = get_user_input()
#     recommendations = recommend_songs(user_songs, data)
#     print("============================= WELCOME TO THE SPOTIFY (MUSICX) ==========================");
#     for song in recommendations:
#         print("\n------------------------------------------------------");
#         print(f"{song['name']} by {song['artists']} ({song['year']})")

# if __name__ == "__main__":
#     main()

"""=================================== NEURAL NEXUS =============================="""
# import streamlit as st
# import pandas as pd
# import numpy as np
# from scipy.spatial.distance import cdist

# # Assuming these functions are defined elsewhere in your code
# def flatten_dict_list(dict_list):
#     # Your implementation here
#     pass

# def get_mean_vector(song_list, spotify_data):
#     # Your implementation here
#     pass

# # Function to recommend songs
# def recommend_songs(song_list, spotify_data, n_songs=10):
#     metadata_cols = ['name', 'year', 'artists']
#     song_dict = flatten_dict_list(song_list)

#     song_center = get_mean_vector(song_list, spotify_data)
#     scaler = song_cluster_pipeline.steps[0][1]
#     scaled_data = scaler.transform(spotify_data[number_cols])
#     scaled_song_center = scaler.transform(song_center.reshape(1, -1))
#     distances = cdist(scaled_song_center, scaled_data, 'cosine')
#     index = list(np.argsort(distances)[:, :n_songs][0])

#     rec_songs = spotify_data.iloc[index]
#     rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]
#     return rec_songs[metadata_cols].to_dict(orient='records')

# # Streamlit app
# def main():
#     st.title("WELCOME TO THE SPOTIFY (MUSICX)")
    
#     st.header("Please register:")
#     name = st.text_input("1. Enter your name:")
#     age = st.number_input("2. Enter your age:", min_value=0)
#     gender = st.selectbox("3. Enter your gender:", ["M", "F"])

#     language_option = st.selectbox("4. Select Language:", ["Hindi", "English", "Other"])
#     language = {
#         'Hindi': 'Hindi',
#         'English': 'English',
#         'Other': 'Other'
#     }.get(language_option, 'Other')

#     song_type_option = st.selectbox("6. Select your preferred song type:", ["Rock", "Romantic", "Neon", "Peace", "Hip-Hop"])
#     song_type = {
#         'Rock': 'Rock',
#         'Romantic': 'Romantic',
#         'Neon': 'Neon',
#         'Peace': 'Peace',
#         'Hip-Hop': 'Hip-Hop'
#     }.get(song_type_option, 'Rock')

#     if st.button("Submit"):
#         st.success("Registered Successfully!")
#     else:
#         st.warning("Not Registered. Only Free Music Available without Premium")
#         return

#     song_list = []
#     st.header("Enter the songs you like:")
#     while True:
#         song_name = st.text_input("Enter the name of a song you like (or type 'done' to finish):")
#         if song_name.lower() == 'done':
#             break
#         artist_name = st.text_input("1. Enter the name of the artist:")
#         song_year = st.number_input("2. Enter the release year of the song:", min_value=0)
#         song_list.append({'name': song_name, 'year': song_year, 'artists': artist_name})

#     if song_list:
#         recommendations = recommend_songs(song_list, data)
#         st.header("Recommended Songs:")
#         for song in recommendations:
#             st.write(f"{song['name']} by {song['artists']} ({song['year']})")

# if __name__ == "__main__":
#     main()

